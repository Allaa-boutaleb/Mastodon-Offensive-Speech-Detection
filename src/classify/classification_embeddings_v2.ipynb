{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification avec Transformer (Encoder seul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et prétraitement des données (seulement en anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des toots(toutes les langues) 93273\n",
      "Nb pos(1): 1569 -- Nb neg(0): 91704 -- total: 93273\n",
      "Taille des toots en anglais 47465\n",
      "Nb pos(1): 1011 -- Nb neg(0): 46454 -- total: 47465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chikmagalur Tourist Places: Your Ultimate Guid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dancing Adélie Penguins, McMurdo Sound, Antar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 Macdonald trip leaving Burrard Station @ Bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here you go seekers, some more good music (the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Future of Nuclear Energy in a Carbon-Const...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  y\n",
       "0   Chikmagalur Tourist Places: Your Ultimate Guid...  0\n",
       "1   Dancing Adélie Penguins, McMurdo Sound, Antar...  0\n",
       "2   2 Macdonald trip leaving Burrard Station @ Bay...  0\n",
       "3   Here you go seekers, some more good music (the...  0\n",
       "11  The Future of Nuclear Energy in a Carbon-Const...  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toots = pd.read_csv('../data/100k_en_toots_labeled.csv').dropna()\n",
    "\n",
    "ys, nb_ys = np.unique(toots['y'], return_counts=True)\n",
    "print('Taille des toots(toutes les langues)', len(toots))\n",
    "print(f'Nb pos({ys[1]}): {nb_ys[1]} -- Nb neg({ys[0]}): {nb_ys[0]} -- total: {nb_ys.sum()}')\n",
    "\n",
    "# toots en anglais\n",
    "itoots_en = toots['language'] == 'en'\n",
    "toots_en = toots[itoots_en]\n",
    "toots_en = toots_en[['content', 'y']]\n",
    "\n",
    "ys, nb_ys = np.unique(toots_en['y'], return_counts=True)\n",
    "print('Taille des toots en anglais', len(toots_en))\n",
    "print(f'Nb pos({ys[1]}): {nb_ys[1]} -- Nb neg({ys[0]}): {nb_ys[0]} -- total: {nb_ys.sum()}')\n",
    "\n",
    "toots_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement et sauvegarde des embeddings (20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072e3d77f838429c976a81ba31496d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Encoder les toots\n",
    "# embeddings = model.encode(toots_en['content'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = toots_en['y'].to_list()\n",
    "# save_object('../data/embedding_100k_en_toots_labeled_eng.dill', [(embi,yi) for embi,yi in zip(embeddings, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les embeddings déjà prétraités et enregistrés\n",
    "\n",
    "mettre les données des embeddings dans: src/data/embedding_100k_en_toots_labeled_eng.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_object('../data/embedding_100k_en_toots_labeled_eng.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx]\n",
    "\n",
    "dataset = MyData(data)\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices, test_indices = next(iter(splitter.split(dataset, [label for _, label in data])))\n",
    "\n",
    "# Use the indices to create training and testing datasets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# Create DataLoaders for iterating over the training and testing sets\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Données équilibrées\n",
    "# equilibrer train\n",
    "ipos = [i for i,(d,y) in enumerate(train_dataset)  if y==1]\n",
    "ineg = [i for i,(d,y) in enumerate(train_dataset)  if y==0]\n",
    "np.random.shuffle(ineg)\n",
    "train_pos = torch.utils.data.Subset(train_dataset, ipos)\n",
    "train_neg = torch.utils.data.Subset(train_dataset, ineg[:len(ipos)])\n",
    "train_dataset_eq = torch.utils.data.ConcatDataset([train_pos,train_neg])\n",
    "# equilibrer test\n",
    "ipos = [i for i,(d,y) in enumerate(test_dataset)  if y==1]\n",
    "ineg = [i for i,(d,y) in enumerate(test_dataset)  if y==0]\n",
    "np.random.shuffle(ineg)\n",
    "train_pos = torch.utils.data.Subset(test_dataset, ipos)\n",
    "train_neg = torch.utils.data.Subset(test_dataset, ineg[:len(ipos)])\n",
    "test_dataset_eq = torch.utils.data.ConcatDataset([train_pos,train_neg])\n",
    "\n",
    "train_dataloader_eq = DataLoader(train_dataset_eq, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_eq = DataLoader(test_dataset_eq, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, dim=384):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    self.main = nn.Sequential(\n",
    "      nn.Linear(dim, 2 * dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(2 * dim, 4 * dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * dim, 2 * dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(2 * dim, dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dim, 1),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n",
    "\n",
    "class BNet(nn.Module):\n",
    "  def __init__(self, dim=384):\n",
    "    super(BNet, self).__init__()\n",
    "    \n",
    "    self.main = nn.Sequential(\n",
    "      nn.Linear(dim, 2 * dim),\n",
    "      nn.BatchNorm2d(dim * 2),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(2 * dim, 4 * dim),\n",
    "      nn.BatchNorm2d(dim * 4),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * dim, 2 * dim),\n",
    "      nn.BatchNorm2d(dim * 2),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(2 * dim, dim),\n",
    "      nn.BatchNorm2d(dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dim, 1),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n",
    "\n",
    "class F1Loss(nn.Module):\n",
    "  def __init__(self, epsilon=1e-7):\n",
    "    super(F1Loss, self).__init__()\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def forward(self, y_true, y_pred):\n",
    "    # Calculez les vrais positifs, faux positifs et faux négatifs\n",
    "    tp = torch.sum(y_true * y_pred)\n",
    "    fp = torch.sum((1 - y_true) * y_pred)\n",
    "    fn = torch.sum(y_true * (1 - y_pred))\n",
    "\n",
    "    # Calculez la précision, le rappel et le F1 score\n",
    "    precision = tp / (tp + fp + self.epsilon)\n",
    "    recall = tp / (tp + fn + self.epsilon)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "\n",
    "    # Utilisez 1 - F1 comme la perte (car PyTorch minimise)\n",
    "    loss = 1 - f1\n",
    "    return loss\n",
    "\n",
    "class F1MacroLoss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(F1MacroLoss, self).__init__()\n",
    "        self.f1_loss = F1Loss()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "      f1_p = 1 - self.f1_loss(y_true, y_pred)\n",
    "      f1_n = 1 - self.f1_loss((1-y_true), (1-y_pred))\n",
    "      # print(f'--F1 score (positifs): {f1_p}')\n",
    "      # print(f'--F1 score (négatifs): {f1_n}')\n",
    "      return 1 - (f1_p + f1_n) / 2\n",
    "\n",
    "class RaucAuc(nn.Module):\n",
    "  def __init__(self, ):\n",
    "    super(RaucAuc, self).__init__()\n",
    "\n",
    "  def forward(self, lyhat, ly):\n",
    "    # Convertir les listes en tenseurs PyTorch\n",
    "    predictions_prob = torch.tensor(predictions_prob)\n",
    "    targets = torch.tensor(targets)\n",
    "\n",
    "    # Utiliser scikit-learn pour calculer l'aire sous la courbe ROC (ROC-AUC)\n",
    "    roc_auc = roc_auc_score(np.array(lyhat), np.array(ly))\n",
    "\n",
    "    loss = 1 - roc_auc\n",
    "    return loss\n",
    "\n",
    "def balanced_accuracy(lyhat, ly):\n",
    "  lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "  ipos, ineg = ly == 1, ly == 0\n",
    "  ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "  lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "  tp = (lyhat_pos == ly_pos).sum()\n",
    "  tn = (lyhat_neg == ly_neg).sum()\n",
    "  \n",
    "  sensitive_p = tp / len(ly_pos)\n",
    "  sensitive_n = tn / len(ly_neg)\n",
    "\n",
    "  ba = (sensitive_p + sensitive_n) / 2\n",
    "  \n",
    "  return ba\n",
    "\n",
    "def f1_macro(lyhat, ly):\n",
    "\n",
    "  # Utiliser scikit-learn pour calculer le F1 score macro\n",
    "  f1_macro = f1_score(np.array(ly), np.array(lyhat), average='macro')\n",
    "\n",
    "  return f1_macro\n",
    "\n",
    "def f_roc_auc(predictions_prob, targets):\n",
    "  # Convertir les listes en tenseurs PyTorch\n",
    "  predictions_prob = torch.tensor(predictions_prob)\n",
    "  targets = torch.tensor(targets)\n",
    "\n",
    "  # Utiliser scikit-learn pour calculer l'aire sous la courbe ROC (ROC-AUC)\n",
    "  roc_auc = roc_auc_score(targets.numpy(), predictions_prob.numpy())\n",
    "\n",
    "  return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apprentissage(net, criterion, optimizer, train_dl, test_dl, epochs):\n",
    "  train_loss, test_loss = [], []\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"Epoch [{(epoch+1):4d}/{epochs:4d}] \")\n",
    "    lep = []\n",
    "    net.train()\n",
    "    for batch_idx, (batch, y) in enumerate(train_dl):\n",
    "\n",
    "      yhat = net(batch).squeeze(dim=1)\n",
    "      y = y.float()\n",
    "      loss = criterion(y, yhat)\n",
    "      \n",
    "      net.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      # save loss train\n",
    "      lep.append(loss.detach().numpy())\n",
    "      \n",
    "      \n",
    "      yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "      ipos, ineg = y == 1, y == 0\n",
    "      y_pos, y_neg = y[ipos], y[ineg]\n",
    "      yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "      if len(y_pos)>0:\n",
    "        pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "      else:\n",
    "        pred_pos = - 0.01\n",
    "      if len(y_neg)>0:\n",
    "        pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "      else:\n",
    "        pred_neg = - 0.01\n",
    "      print(\n",
    "        f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dl):4d}] - \"\n",
    "        f\"Loss : {loss:.4f} - \"\n",
    "        f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "        f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "      )\n",
    "    print(\n",
    "      f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dl):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "    )\n",
    "    train_loss.append(np.mean(lep))\n",
    "    \n",
    "    lep = []\n",
    "    net.eval()\n",
    "    for batch_idx, (batch, y) in enumerate(test_dl):\n",
    "\n",
    "      yhat = net(batch).squeeze(dim=1)\n",
    "      loss = criterion(y, yhat)\n",
    "      lep.append(loss.detach().numpy())\n",
    "      \n",
    "      yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "      ipos, ineg = y == 1, y == 0\n",
    "      y_pos, y_neg = y[ipos], y[ineg]\n",
    "      yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "      if len(y_pos)>0:\n",
    "        pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "      if len(y_neg)>0:\n",
    "        pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "      print(\n",
    "        f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dl):4d}] - \"\n",
    "        f\"Loss : {loss:.4f} - \"\n",
    "        f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "        f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "      )\n",
    "    print(\n",
    "      f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dl):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "    )\n",
    "    test_loss.append(np.mean(lep))\n",
    "  \n",
    "  return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(net, dl):\n",
    "  net.eval()\n",
    "  lyhat, ly = [], []\n",
    "  for batch_idx, (batch, y) in enumerate(dl):\n",
    "    \n",
    "    yhat = net(batch).squeeze(dim=1)\n",
    "    ly.extend(y)\n",
    "    lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "  lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "  ipos, ineg = ly == 1, ly == 0\n",
    "  ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "  lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "  print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "  print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "  print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "  \n",
    "  f1_loss = F1Loss()\n",
    "  print(f'F1 score (positifs): {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "  print(f'F1 score (négatifs): {(100*(1-f1_loss(1-ly, 1-lyhat))):.2f}%')\n",
    "  \n",
    "  f1_macro_loss = F1MacroLoss()\n",
    "  sc = 1-f1_macro_loss(ly, lyhat)\n",
    "  print(f'f1 macro score : {(100*sc):.2f}%')\n",
    "\n",
    "  # scores\n",
    "  tp = (lyhat_pos == ly_pos).sum()\n",
    "  tn = (lyhat_neg == ly_neg).sum()\n",
    "  fp = (lyhat_pos != ly_pos).sum()\n",
    "  fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "  accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "  precision_score = tp / (tp + fp)\n",
    "  recall_score = tp / (tp + fn)\n",
    "  f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "  print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "  print(f'precision score: {(100*precision_score):.2f}%')\n",
    "  print(f'recall score: {(100*recall_score):.2f}%')\n",
    "  print(f'f1 score: {(100*f1_score):.2f}%')\n",
    "  \n",
    "  return lyhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage avec des classes non équilibrées avec MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\n",
    "# criterion = F1Loss()\n",
    "criterion = F1MacroLoss()\n",
    "\n",
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.5000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5120 - TP/P : 0.00 - TN/N : 95.24070\n",
      "Epoch [   2/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.5000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   3/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.5000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5004 - TP/P : 0.00 - TN/N : 100.0050\n",
      "Epoch [   4/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.5128 - TP/P : -1.00 - TN/N : 95.0050\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5001 - TP/P : 100.00 - TN/N : 100.00\n",
      "Epoch [   5/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0000 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5000 - TP/P : 0.00 - TN/N : 100.0050\n",
      "Epoch [   6/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.5000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5000 - TP/P : 0.00 - TN/N : 100.0070\n",
      "Epoch [   7/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0001 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [ 297/ 297] - Loss : 0.5000 - TP/P : 0.00 - TN/N : 100.0000\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = apprentissage(net, criterion, optimizer, train_dataloader, test_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 97.72%\n",
      "Taux de prediction sur les pos: 10.88%\n",
      "Taux de prediction sur les neg: 99.61%\n",
      "F1 score (positifs): 16.87%\n",
      "F1 score (négatifs): 98.84%\n",
      "f1 macro score : 57.86%\n",
      "\n",
      "accuracy score: 97.72%\n",
      "precision score: 10.88%\n",
      "recall score: 37.61%\n",
      "f1 score: 16.87%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 97.74%\n",
      "Taux de prediction sur les pos: 11.39%\n",
      "Taux de prediction sur les neg: 99.61%\n",
      "F1 score (positifs): 17.62%\n",
      "F1 score (négatifs): 98.85%\n",
      "f1 macro score : 58.24%\n",
      "\n",
      "accuracy score: 97.74%\n",
      "precision score: 11.39%\n",
      "recall score: 38.98%\n",
      "f1 score: 17.62%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage avec des classes non équilibrées avec f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f1 = Net()\n",
    "optimizer_f1 = torch.optim.Adam(net_f1.parameters(), lr=3e-4)\n",
    "criterion_f1 = F1Loss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   2/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   3/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : -1.00 - TN/N : 95.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   4/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   5/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   6/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : -1.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   7/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.5000 - TP/P : 33.33 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = apprentissage(net_f1, criterion_f1, optimizer_f1, train_dataloader, test_dataloader, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 96.22%\n",
      "Taux de prediction sur les pos: 26.45%\n",
      "Taux de prediction sur les neg: 97.74%\n",
      "F1 score: 22.97%\n",
      "f1 macro score: 39.48%\n",
      "\n",
      "accuracy score: 96.22%\n",
      "precision score: 26.45%\n",
      "recall score: 20.30%\n",
      "f1 score: 22.97%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net_f1, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 96.05%\n",
      "Taux de prediction sur les pos: 22.28%\n",
      "Taux de prediction sur les neg: 97.65%\n",
      "F1 score: 19.35%\n",
      "f1 macro score: 41.33%\n",
      "\n",
      "accuracy score: 96.05%\n",
      "precision score: 22.28%\n",
      "recall score: 17.11%\n",
      "f1 score: 19.35%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net_f1, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage avec des classes équilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_eq = Net()\n",
    "optimizer_eq = torch.optim.Adam(net_eq.parameters(), lr=3e-4)\n",
    "# criterion = F1Loss()\n",
    "criterion_eq = nn.MSELoss()\n",
    "\n",
    "epochs = 7\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.1766 - TP/P : 83.33 - TN/N : 66.672\n",
      "\ttest batch [  13/  13] - Loss : 0.1479 - TP/P : 90.00 - TN/N : 85.00\n",
      "Epoch [   2/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.1552 - TP/P : 90.00 - TN/N : 62.5000\n",
      "\ttest batch [  13/  13] - Loss : 0.1224 - TP/P : 80.00 - TN/N : 85.00\n",
      "Epoch [   3/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0686 - TP/P : 100.00 - TN/N : 90.000\n",
      "\ttest batch [  13/  13] - Loss : 0.1895 - TP/P : 90.00 - TN/N : 80.00\n",
      "Epoch [   4/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.1156 - TP/P : 100.00 - TN/N : 75.000\n",
      "\ttest batch [  13/  13] - Loss : 0.1876 - TP/P : 80.00 - TN/N : 80.00\n",
      "Epoch [   5/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0010 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [  13/  13] - Loss : 0.2054 - TP/P : 80.00 - TN/N : 80.000\n",
      "Epoch [   6/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0558 - TP/P : 100.00 - TN/N : 90.000\n",
      "\ttest batch [  13/  13] - Loss : 0.2643 - TP/P : 80.00 - TN/N : 65.00\n",
      "Epoch [   7/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0003 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [  13/  13] - Loss : 0.2433 - TP/P : 80.00 - TN/N : 75.000\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = apprentissage(net_eq, criterion_eq, optimizer_eq, train_dataloader_eq, test_dataloader_eq, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train avec équilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 98.45%\n",
      "Taux de prediction sur les pos: 98.76%\n",
      "Taux de prediction sur les neg: 98.15%\n",
      "F1 score (positifs): 98.46%\n",
      "F1 score (négatifs): 98.45%\n",
      "f1 macro score : 1.55%\n",
      "\n",
      "accuracy score: 98.45%\n",
      "precision score: 98.76%\n",
      "recall score: 98.16%\n",
      "f1 score: 98.46%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net_eq, train_dataloader_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 75.12%\n",
      "Taux de prediction sur les pos: 98.76%\n",
      "Taux de prediction sur les neg: 74.61%\n",
      "F1 score (positifs): 14.47%\n",
      "F1 score (négatifs): 85.44%\n",
      "f1 macro score : 50.04%\n",
      "\n",
      "accuracy score: 75.12%\n",
      "precision score: 98.76%\n",
      "recall score: 7.81%\n",
      "f1 score: 14.47%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net_eq, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test avec equilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 80.20%\n",
      "Taux de prediction sur les pos: 87.13%\n",
      "Taux de prediction sur les neg: 73.27%\n",
      "F1 score: 81.48%\n",
      "f1 macro score: 19.90%\n",
      "\n",
      "accuracy score: 80.20%\n",
      "precision score: 87.13%\n",
      "recall score: 76.52%\n",
      "f1 score: 81.48%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net_eq, test_dataloader_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 72.38%\n",
      "Taux de prediction sur les pos: 87.62%\n",
      "Taux de prediction sur les neg: 72.05%\n",
      "F1 score: 11.90%\n",
      "\n",
      "accuracy score: 72.38%\n",
      "precision score: 87.62%\n",
      "recall score: 6.38%\n",
      "f1 score: 11.90%\n"
     ]
    }
   ],
   "source": [
    "lyhat = prediction(net_eq, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
