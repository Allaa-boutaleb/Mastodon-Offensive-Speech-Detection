{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification avec Transformer (Encoder seul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et prétraitement des données (seulement en anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des toots(toutes les langues) 93273\n",
      "Nb pos(1): 1569 -- Nb neg(0): 91704 -- total: 93273\n",
      "Taille des toots en anglais 47465\n",
      "Nb pos(1): 1011 -- Nb neg(0): 46454 -- total: 47465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chikmagalur Tourist Places: Your Ultimate Guid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dancing Adélie Penguins, McMurdo Sound, Antar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 Macdonald trip leaving Burrard Station @ Bay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here you go seekers, some more good music (the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Future of Nuclear Energy in a Carbon-Const...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  y\n",
       "0   Chikmagalur Tourist Places: Your Ultimate Guid...  0\n",
       "1   Dancing Adélie Penguins, McMurdo Sound, Antar...  0\n",
       "2   2 Macdonald trip leaving Burrard Station @ Bay...  0\n",
       "3   Here you go seekers, some more good music (the...  0\n",
       "11  The Future of Nuclear Energy in a Carbon-Const...  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toots = pd.read_csv('../data/100k_en_toots_labeled.csv').dropna()\n",
    "\n",
    "ys, nb_ys = np.unique(toots['y'], return_counts=True)\n",
    "print('Taille des toots(toutes les langues)', len(toots))\n",
    "print(f'Nb pos({ys[1]}): {nb_ys[1]} -- Nb neg({ys[0]}): {nb_ys[0]} -- total: {nb_ys.sum()}')\n",
    "\n",
    "# toots en anglais\n",
    "itoots_en = toots['language'] == 'en'\n",
    "toots_en = toots[itoots_en]\n",
    "toots_en = toots_en[['content', 'y']]\n",
    "\n",
    "ys, nb_ys = np.unique(toots_en['y'], return_counts=True)\n",
    "print('Taille des toots en anglais', len(toots_en))\n",
    "print(f'Nb pos({ys[1]}): {nb_ys[1]} -- Nb neg({ys[0]}): {nb_ys[0]} -- total: {nb_ys.sum()}')\n",
    "\n",
    "toots_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement et sauvegarde des embeddings (20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072e3d77f838429c976a81ba31496d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Encoder les toots\n",
    "# embeddings = model.encode(toots_en['content'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = toots_en['y'].to_list()\n",
    "# save_object('../data/embedding_100k_en_toots_labeled_eng.dill', [(embi,yi) for embi,yi in zip(embeddings, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les embeddings déjà prétraités et enregistrés\n",
    "\n",
    "mettre les données des embeddings dans: src/data/embedding_100k_en_toots_labeled_eng.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_object('../data/embedding_100k_en_toots_labeled_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx]\n",
    "\n",
    "dataset = MyData(data)\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices, test_indices = next(iter(splitter.split(dataset, [label for _, label in data])))\n",
    "\n",
    "# Use the indices to create training and testing datasets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# equilibrer train\n",
    "ipos = [i for i,(d,y) in enumerate(train_dataset)  if y==1]\n",
    "ineg = [i for i,(d,y) in enumerate(train_dataset)  if y==0]\n",
    "np.random.shuffle(ineg)\n",
    "train_pos = torch.utils.data.Subset(train_dataset, ipos)\n",
    "train_neg = torch.utils.data.Subset(train_dataset, ineg[:len(ipos)])\n",
    "train_dataset_eq = torch.utils.data.ConcatDataset([train_pos,train_neg])\n",
    "# equilibrer test\n",
    "ipos = [i for i,(d,y) in enumerate(test_dataset)  if y==1]\n",
    "ineg = [i for i,(d,y) in enumerate(test_dataset)  if y==0]\n",
    "np.random.shuffle(ineg)\n",
    "train_pos = torch.utils.data.Subset(test_dataset, ipos)\n",
    "train_neg = torch.utils.data.Subset(test_dataset, ineg[:len(ipos)])\n",
    "test_dataset_eq = torch.utils.data.ConcatDataset([train_pos,train_neg])\n",
    "\n",
    "# Create DataLoaders for iterating over the training and testing sets\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_eq = DataLoader(train_dataset_eq, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_eq = DataLoader(test_dataset_eq, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, dim=384):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    self.main = nn.Sequential(\n",
    "      nn.Linear(dim, 2 * dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(2 * dim, 4 * dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * dim, 2 * dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(2 * dim, dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dim, 1),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n",
    "\n",
    "class F1Loss(nn.Module):\n",
    "  def __init__(self, epsilon=1e-7):\n",
    "    super(F1Loss, self).__init__()\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def forward(self, y_true, y_pred):\n",
    "    # Calculez les vrais positifs, faux positifs et faux négatifs\n",
    "    tp = torch.sum(y_true * y_pred)\n",
    "    fp = torch.sum((1 - y_true) * y_pred)\n",
    "    fn = torch.sum(y_true * (1 - y_pred))\n",
    "\n",
    "    # Calculez la précision, le rappel et le F1 score\n",
    "    precision = tp / (tp + fp + self.epsilon)\n",
    "    recall = tp / (tp + fn + self.epsilon)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "\n",
    "    # Utilisez 1 - F1 comme la perte (car PyTorch minimise)\n",
    "    loss = 1 - f1\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage avec des classes non équilibrées avec MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\n",
    "# criterion = F1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 7\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0056 - TP/P : 0.00 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0017 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   2/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0001 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0003 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   3/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0023 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   4/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0000 - TP/P : 33.33 - TN/N : 100.000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   5/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0094 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   6/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0013 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   7/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 0.0018 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [ 297/ 297] - Loss : 0.0001 - TP/P : 0.00 - TN/N : 100.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch [{(epoch+1):4d}/{epochs:4d}] \")\n",
    "  lep = []\n",
    "  net.train()\n",
    "  for batch_idx, (batch, y) in enumerate(train_dataloader):\n",
    "\n",
    "    yhat = net(batch).squeeze(dim=1)\n",
    "    y = y.float()\n",
    "    loss = criterion(y, yhat)\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # save loss train\n",
    "    lep.append(loss.detach().numpy())\n",
    "    \n",
    "    \n",
    "    yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "    ipos, ineg = y == 1, y == 0\n",
    "    y_pos, y_neg = y[ipos], y[ineg]\n",
    "    yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "    if len(y_pos)>0:\n",
    "      pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "    if len(y_neg)>0:\n",
    "      pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "    print(\n",
    "      f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dataloader):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "    )\n",
    "  print(\n",
    "    f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dataloader):4d}] - \"\n",
    "    f\"Loss : {loss:.4f} - \"\n",
    "    f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "    f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "  )\n",
    "  train_loss.append(np.mean(lep))\n",
    "  \n",
    "  lep = []\n",
    "  net.eval()\n",
    "  for batch_idx, (batch, y) in enumerate(test_dataloader):\n",
    "\n",
    "    yhat = net(batch).squeeze(dim=1)\n",
    "    loss = criterion(y, yhat)\n",
    "    lep.append(loss.detach().numpy())\n",
    "    \n",
    "    yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "    ipos, ineg = y == 1, y == 0\n",
    "    y_pos, y_neg = y[ipos], y[ineg]\n",
    "    yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "    if len(y_pos)>0:\n",
    "      pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "    if len(y_neg)>0:\n",
    "      pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "    print(\n",
    "      f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dataloader):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "    )\n",
    "  print(\n",
    "    f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dataloader):4d}] - \"\n",
    "    f\"Loss : {loss:.4f} - \"\n",
    "    f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "    f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "  )\n",
    "  test_loss.append(np.mean(lep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 99.10%\n",
      "Taux de prediction sur les pos: 68.85%\n",
      "Taux de prediction sur les neg: 99.76%\n",
      "F1 score: 76.46%\n",
      "\n",
      "accuracy score: 99.10%\n",
      "precision score: 68.85%\n",
      "recall score: 85.96%\n",
      "f1 score: 76.46%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(train_dataloader):\n",
    "  \n",
    "  yhat = net(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 96.99%\n",
      "Taux de prediction sur les pos: 28.71%\n",
      "Taux de prediction sur les neg: 98.47%\n",
      "F1 score: 28.86%\n",
      "\n",
      "accuracy score: 96.99%\n",
      "precision score: 28.71%\n",
      "recall score: 29.00%\n",
      "f1 score: 28.86%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(test_dataloader):\n",
    "  \n",
    "  yhat = net(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage avec des classes non équilibrées avec f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f1 = Net()\n",
    "optimizer = torch.optim.Adam(net_f1.parameters(), lr=3e-4)\n",
    "criterion = F1Loss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 7\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24050\n",
      "Epoch [   2/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.00320\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 95.24000\n",
      "Epoch [   3/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 100.00 - TN/N : 95.000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0030\n",
      "Epoch [   4/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0070\n",
      "Epoch [   5/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   6/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "Epoch [   7/   7] \n",
      "\ttrain batch [1187/1187] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0000\n",
      "\ttest batch [ 297/ 297] - Loss : 1.0000 - TP/P : 0.00 - TN/N : 100.0050\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch [{(epoch+1):4d}/{epochs:4d}] \")\n",
    "  lep = []\n",
    "  net_f1.train()\n",
    "  for batch_idx, (batch, y) in enumerate(train_dataloader):\n",
    "\n",
    "    yhat = net_f1(batch).squeeze(dim=1)\n",
    "    y = y.float()\n",
    "    loss = criterion(y, yhat)\n",
    "    \n",
    "    net_f1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # save loss train\n",
    "    lep.append(loss.detach().numpy())\n",
    "    \n",
    "    \n",
    "    yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "    ipos, ineg = y == 1, y == 0\n",
    "    y_pos, y_neg = y[ipos], y[ineg]\n",
    "    yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "    if len(y_pos)>0:\n",
    "      pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "    if len(y_neg)>0:\n",
    "      pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "    print(\n",
    "      f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dataloader):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "    )\n",
    "  print(\n",
    "    f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dataloader):4d}] - \"\n",
    "    f\"Loss : {loss:.4f} - \"\n",
    "    f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "    f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "  )\n",
    "  train_loss.append(np.mean(lep))\n",
    "  \n",
    "  lep = []\n",
    "  net_f1.eval()\n",
    "  for batch_idx, (batch, y) in enumerate(test_dataloader):\n",
    "\n",
    "    yhat = net_f1(batch).squeeze(dim=1)\n",
    "    loss = criterion(y, yhat)\n",
    "    lep.append(loss.detach().numpy())\n",
    "    \n",
    "    yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "    ipos, ineg = y == 1, y == 0\n",
    "    y_pos, y_neg = y[ipos], y[ineg]\n",
    "    yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "    if len(y_pos)>0:\n",
    "      pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "    if len(y_neg)>0:\n",
    "      pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "    print(\n",
    "      f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dataloader):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "    )\n",
    "  print(\n",
    "    f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dataloader):4d}] - \"\n",
    "    f\"Loss : {loss:.4f} - \"\n",
    "    f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "    f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "  )\n",
    "  test_loss.append(np.mean(lep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 96.50%\n",
      "Taux de prediction sur les pos: 28.55%\n",
      "Taux de prediction sur les neg: 97.98%\n",
      "F1 score: 25.80%\n",
      "\n",
      "accuracy score: 96.50%\n",
      "precision score: 28.55%\n",
      "recall score: 23.52%\n",
      "f1 score: 25.80%\n"
     ]
    }
   ],
   "source": [
    "net_f1.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(train_dataloader):\n",
    "  \n",
    "  yhat = net_f1(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 96.23%\n",
      "Taux de prediction sur les pos: 23.76%\n",
      "Taux de prediction sur les neg: 97.80%\n",
      "F1 score: 21.15%\n",
      "\n",
      "accuracy score: 96.23%\n",
      "precision score: 23.76%\n",
      "recall score: 19.05%\n",
      "f1 score: 21.15%\n"
     ]
    }
   ],
   "source": [
    "net_f1.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(test_dataloader):\n",
    "  \n",
    "  yhat = net_f1(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage avec des classes équilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_eq = Net()\n",
    "optimizer = torch.optim.Adam(net_eq.parameters(), lr=3e-4)\n",
    "# criterion = F1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 7\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.1205 - TP/P : 100.00 - TN/N : 81.82\n",
      "\ttest batch [  13/  13] - Loss : 0.0780 - TP/P : 90.00 - TN/N : 90.00\n",
      "Epoch [   2/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0956 - TP/P : 90.00 - TN/N : 87.500\n",
      "\ttest batch [  13/  13] - Loss : 0.1066 - TP/P : 90.00 - TN/N : 85.00\n",
      "Epoch [   3/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0756 - TP/P : 100.00 - TN/N : 88.890\n",
      "\ttest batch [  13/  13] - Loss : 0.0703 - TP/P : 90.00 - TN/N : 90.00\n",
      "Epoch [   4/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0362 - TP/P : 100.00 - TN/N : 88.890\n",
      "\ttest batch [  13/  13] - Loss : 0.1296 - TP/P : 90.00 - TN/N : 85.00\n",
      "Epoch [   5/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0030 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [  13/  13] - Loss : 0.1321 - TP/P : 90.00 - TN/N : 85.000\n",
      "Epoch [   6/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.1336 - TP/P : 77.78 - TN/N : 88.8900\n",
      "\ttest batch [  13/  13] - Loss : 0.0840 - TP/P : 70.00 - TN/N : 90.00\n",
      "Epoch [   7/   7] \n",
      "\ttrain batch [  51/  51] - Loss : 0.0007 - TP/P : 100.00 - TN/N : 100.00\n",
      "\ttest batch [  13/  13] - Loss : 0.1738 - TP/P : 90.00 - TN/N : 80.000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch [{(epoch+1):4d}/{epochs:4d}] \")\n",
    "  lep = []\n",
    "  net_eq.train()\n",
    "  for batch_idx, (batch, y) in enumerate(train_dataloader_eq):\n",
    "\n",
    "    yhat = net_eq(batch).squeeze(dim=1)\n",
    "    y = y.float()\n",
    "    loss = criterion(y, yhat)\n",
    "    \n",
    "    net_eq.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # save loss train\n",
    "    lep.append(loss.detach().numpy())\n",
    "    \n",
    "    \n",
    "    yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "    ipos, ineg = y == 1, y == 0\n",
    "    y_pos, y_neg = y[ipos], y[ineg]\n",
    "    yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "    if len(y_pos)>0:\n",
    "      pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "    if len(y_neg)>0:\n",
    "      pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "    print(\n",
    "      f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dataloader_eq):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "    )\n",
    "  print(\n",
    "    f\"\\ttrain batch [{(batch_idx+1):4d}/{len(train_dataloader_eq):4d}] - \"\n",
    "    f\"Loss : {loss:.4f} - \"\n",
    "    f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "    f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "  )\n",
    "  train_loss.append(np.mean(lep))\n",
    "  \n",
    "  lep = []\n",
    "  net_eq.eval()\n",
    "  for batch_idx, (batch, y) in enumerate(test_dataloader_eq):\n",
    "\n",
    "    yhat = net_eq(batch).squeeze(dim=1)\n",
    "    loss = criterion(y, yhat)\n",
    "    lep.append(loss.detach().numpy())\n",
    "    \n",
    "    yhat_label = torch.tensor([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "    ipos, ineg = y == 1, y == 0\n",
    "    y_pos, y_neg = y[ipos], y[ineg]\n",
    "    yhat_pos, yhat_neg = yhat_label[ipos], yhat_label[ineg]\n",
    "    if len(y_pos)>0:\n",
    "      pred_pos = (yhat_pos == y_pos).sum()/ len(y_pos)\n",
    "    if len(y_neg)>0:\n",
    "      pred_neg = (yhat_neg == y_neg).sum()/ len(y_neg)\n",
    "    print(\n",
    "      f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dataloader_eq):4d}] - \"\n",
    "      f\"Loss : {loss:.4f} - \"\n",
    "      f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "      f\"TN/N : {(100*pred_neg):.2f}\", end=\"\\r\"\n",
    "    )\n",
    "  print(\n",
    "    f\"\\ttest batch [{(batch_idx+1):4d}/{len(test_dataloader_eq):4d}] - \"\n",
    "    f\"Loss : {loss:.4f} - \"\n",
    "    f\"TP/P : {(100*pred_pos):.2f} - \"\n",
    "    f\"TN/N : {(100*pred_neg):.2f}\"\n",
    "  )\n",
    "  test_loss.append(np.mean(lep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train avec équilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 98.21%\n",
      "Taux de prediction sur les pos: 98.39%\n",
      "Taux de prediction sur les neg: 98.02%\n",
      "F1 score: 98.21%\n",
      "\n",
      "accuracy score: 98.21%\n",
      "precision score: 98.39%\n",
      "recall score: 98.03%\n",
      "f1 score: 98.21%\n"
     ]
    }
   ],
   "source": [
    "net_eq.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(train_dataloader_eq):\n",
    "  \n",
    "  yhat = net_eq(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 73.38%\n",
      "Taux de prediction sur les pos: 98.39%\n",
      "Taux de prediction sur les neg: 72.84%\n",
      "F1 score: 13.61%\n",
      "\n",
      "accuracy score: 73.38%\n",
      "precision score: 98.39%\n",
      "recall score: 7.31%\n",
      "f1 score: 13.61%\n"
     ]
    }
   ],
   "source": [
    "net_eq.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(train_dataloader):\n",
    "  \n",
    "  yhat = net_eq(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test avec equilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 80.45%\n",
      "Taux de prediction sur les pos: 87.62%\n",
      "Taux de prediction sur les neg: 73.27%\n",
      "F1 score: 81.76%\n",
      "\n",
      "accuracy score: 80.45%\n",
      "precision score: 87.62%\n",
      "recall score: 76.62%\n",
      "f1 score: 81.76%\n"
     ]
    }
   ],
   "source": [
    "net_eq.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(test_dataloader_eq):\n",
    "  \n",
    "  yhat = net_eq(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de bonnes prédiction dans test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de prediction total: 72.38%\n",
      "Taux de prediction sur les pos: 87.62%\n",
      "Taux de prediction sur les neg: 72.05%\n",
      "F1 score: 11.90%\n",
      "\n",
      "accuracy score: 72.38%\n",
      "precision score: 87.62%\n",
      "recall score: 6.38%\n",
      "f1 score: 11.90%\n"
     ]
    }
   ],
   "source": [
    "net_eq.eval()\n",
    "lyhat, ly = [], []\n",
    "for batch_idx, (batch, y) in enumerate(test_dataloader):\n",
    "  \n",
    "  yhat = net_eq(batch).squeeze(dim=1)\n",
    "  ly.extend(y)\n",
    "  lyhat.extend([1 if yihat > 0.5 else 0 for yihat in yhat])\n",
    "\n",
    "lyhat, ly = torch.tensor(lyhat), torch.tensor(ly)\n",
    "\n",
    "ipos, ineg = ly == 1, ly == 0\n",
    "ly_pos, ly_neg = ly[ipos], ly[ineg]\n",
    "lyhat_pos, lyhat_neg = lyhat[ipos], lyhat[ineg]\n",
    "\n",
    "print(f'Taux de prediction total: {(100*((lyhat == ly).sum()/ len(ly))):.2f}%')\n",
    "print(f'Taux de prediction sur les pos: {(100*((lyhat_pos == ly_pos).sum()/ len(ly_pos))):.2f}%')\n",
    "print(f'Taux de prediction sur les neg: {(100*((lyhat_neg == ly_neg).sum()/ len(ly_neg))):.2f}%')\n",
    "f1_loss = F1Loss()\n",
    "print(f'F1 score: {(100*(1-f1_loss(ly, lyhat))):.2f}%')\n",
    "\n",
    "# scores\n",
    "tp = (lyhat_pos == ly_pos).sum()\n",
    "tn = (lyhat_neg == ly_neg).sum()\n",
    "fp = (lyhat_pos != ly_pos).sum()\n",
    "fn = (lyhat_neg != ly_neg).sum()\n",
    "\n",
    "accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_score = tp / (tp + fp)\n",
    "recall_score = tp / (tp + fn)\n",
    "f1_score = 2*precision_score*recall_score / (precision_score + recall_score)\n",
    "\n",
    "print(f'\\naccuracy score: {(100*accuracy_score):.2f}%')\n",
    "print(f'precision score: {(100*precision_score):.2f}%')\n",
    "print(f'recall score: {(100*recall_score):.2f}%')\n",
    "print(f'f1 score: {(100*f1_score):.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
